<!DOCTYPE html>
<html>
  <head>
    <title>WACV 2021 Tutorial</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="./css/main.css">
  </head>

  <body>
<div class="container">
  <table border="0" align="center">
    <tbody><tr>
      <td width="1000" align="center" valign="middle"><h1>Audio-Visual Scene Understanding</h1>
        <h3>WACV 2021 Tutorial</h3></td>
    </tr>
    <tr>
      <td colspan="6" align="center"><h4><a href="#" target="_blank"> Click here to join our tutorial </a></h4></td>
    </tr>
    <tr>
      <td colspan="4" align="center"> <h5>Time: #</h5></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h5> Location: #</h5></td>
    </tr>
    </tbody></table>
</div>

<br>
    
<!-- <div class="container">
  <div class="imgDiv"><img class="cvprImg" src="https://convaistorageui.blob.core.windows.net/images/cvpr_tutorial/CVPR_image.png" /></div>
</div> -->

<br>

<div class="container">
  <h2>Overview</h2>
    <p>Sight and hearing are two of the most important senses for human perception. From cognitive perspective, the visual and auditory information is actually slightly discrepant, but the percept is unified with multisensory integration. Whatâ€™s more, when there are multiple input senses, human reactions usually perform more exactly or efficiently than single sense. Inspired by this, for computational models, our community has begun to explore marrying computer vision with audition, and targets to address some essential problems of audio-visual learning then further develops them into interesting and worthwhile tasks. In recent years, we were delighted to witness many developments in learning from both visual and auditory data.
    </p>
    <p>This tutorial aims to cover recent advances in audio-visual learning, including <strong>audio-visual self-supervised learning</strong>, <strong>audio-visual sound separation</strong>, <strong>audio-visual cross-modal generation</strong>, and <strong>audio-visual video understanding</strong>. For each research sub-topic, we will give a concrete introduction of the contained problems/tasks, and the current research progress as well as the open problems. We hope the audience, not only the graduate students but also the researchers new in this area, can benefit from this tutorial and learn the principle problems and cutting-edge approaches of audio-visual learning.
    </p>
</div>

<br>

<div class="container">
  <h2>Agenda</h2>
    <div class="schedule" style="text-align: left;">
      <p><em>8:30 - 8:35 </em> &nbsp;&nbsp;&nbsp;&nbsp;<strong> Welcome </strong> </p>
      <p><em>8:35 - 9:20 </em> &nbsp;&nbsp;&nbsp;&nbsp;<strong> Audio-Visual Self-supervised Learning </strong> </p>
      <p><em>9:20 - 10:05 </em> &nbsp;&nbsp;&nbsp;<strong> Audio-Visual Sound Separation </strong> </p>
      <p><em>10:05 - 10:15 </em> &nbsp;&nbsp;<strong> Coffee Break </strong> </p>
      <p><em>10:15 - 11:00 </em> &nbsp;&nbsp;<strong> Audio-Visual Cross-modal Generation </strong> </p>
      <p><em>11:00 - 11:45 </em> &nbsp;&nbsp;<strong> Audio-Visual Video Understanding </strong> </p>
      <p><em>11:45 - 12:00 </em> &nbsp;&nbsp;<strong> Closing Remarks </strong> </p>
      <!-- <p><em>8:30 - 8:35 </em> &nbsp;&nbsp;<strong> Opening Remarks </strong> presented by <a href="https://www.linkedin.com/in/jingjing-liu-65703431/" target="_blank"> JJ Liu </a> and <a href="https://www.linkedin.com/in/xiaodonghe/" target="_blank"> Xiaodong He </a> (<a href="./slides/tutorial-part-1-opening.pdf" target="_blank"> -->
    </div>
</div>
    
<br/>
    
<div class="container">
  <h2>Organizers</h2>
    <div class="row">
      <div class="parent col-md-3"> 
         <a href="https://dtaoo.github.io/" target="_blank">
         <div class="imgDiv"><img class="profileDescriptionImg" src="https://i.postimg.cc/N03F4nrD/hudi.jpg"/></div>
         <div class="nameText"> Di Hu </div></a>
      </div>
      
      <div class="parent col-md-3"> 
         <a href="http://yapengtian.org/" target="_blank">
         <div class="imgDiv"><img class="profileDescriptionImg" src="https://i.postimg.cc/cJg6fW66/yapeng.jpg"/></div>
         <div class="nameText"> Licheng Yu </div></a>
      </div>
      
      <div class="parent col-md-3"> 
         <a href="https://www.cs.rochester.edu/u/lchen63/" target="_blank">
         <div class="imgDiv"><img class="profileDescriptionImg" src="https://i.postimg.cc/SNMZ125G/Lele.jpg"/></div>
         <div class="nameText"> Lele Chen </div></a>
      </div>
      <div class="parent col-md-3">
         <a href="https://www.cs.rochester.edu/~cxu22/" target="_blank">
         <div class="imgDiv"><img class="profileDescriptionImg" src="https://i.postimg.cc/Cx6zZ5hW/cxu.jpg"/></div>
         <div class="nameText"> Chenliang Xu</div></a>
      </div>
      <br>
      <br>
      
    </div>
      
  
</div>

<br>

<!-- <div class="container">
  <p>Contact: <a href="mailto:Zhe.Gan@microsoft.com">Zhe Gan</a> for more details/questions</p>
  <p style="text-align: right">Website made by <a href="https://www.linkedin.com/in/rohitpillai97/" target="_blank">Rohit Pillai</a></p>
</div> -->

<!--<p align="center" class="acknowledgement">Last updated: 30 July 2012</p>-->

<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body></html>
